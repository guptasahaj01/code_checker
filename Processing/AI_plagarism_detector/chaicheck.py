import re
import os
import sys
import glob
import json
import numpy as np
from typing import Dict, List, Tuple, Optional, Set, Any
from collections import defaultdict, Counter
import pickle

try:
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.feature_extraction.text import TfidfVectorizer
    SKLEARN_AVAILABLE = True
except ImportError:
    print("Warning: scikit-learn not installed. ML-based detection disabled.")
    SKLEARN_AVAILABLE = False

# Example AI snippets for TF-IDF comparison
AI_CODE_SNIPPETS = [
    """
    // Function to calculate the sum of two numbers
    // This function takes two integers as input and returns their sum
    int calculateSum(int a, int b) {
        return a + b;
    }
    """,
    
    """
    // Function to validate user input
    // Returns true if the input is valid, false otherwise
    bool validateInput(const std::string& input) {
        if (input.empty()) {
            return false;
        }
        // Check for special characters
        for (char c : input) {
            if (!isalnum(c) && c != ' ') {
                return false;
            }
        }
        return true;
    }
    """,
    
    """
    // Utility class for handling file operations
    class FileHandler {
    private:
        std::string filePath;
        std::fstream fileStream;
    
    public:
        // Constructor initializes the file path
        FileHandler(const std::string& path) : filePath(path) {}
        
        // Opens the file for reading or writing
        bool openFile(bool writeMode = false) {
            fileStream.open(filePath, writeMode ? std::ios::out : std::ios::in);
            return fileStream.is_open();
        }
        
        // Closes the file
        void closeFile() {
            if (fileStream.is_open()) {
                fileStream.close();
            }
        }
    };
    """
]

class CppAIDetector:
    def __init__(self):
        # C++ specific AI generation patterns
        self.cpp_ai_patterns = [
            r'// Generated by',
            r'// Created by',
            r'// Auto-generated',
            r'// AI-generated',
            r'// Generated using',
            r'// Created using',
            r'// Generated with',
            r'// Created with',
            r'// Generated from',
            r'// Created from',
            r'// Generated via',
            r'// Created via',
            r'// Generated through',
            r'// Created through',
            r'// This code was',
            r'// This implementation was',
            r'// This solution was',
            r'// This algorithm was',
            r'// Code written by AI',
            r'// Created with assistance from', 
            r'// Helped by',
            r'// Adapted from',
            r'// Based on prompt',
            r'// Generated in response to',
            r'// Automatically implemented',
            r'// Solved using',
            r'// Programmed by',
            r'// .* to initialize .*',  # Common in explanatory comments
            r'// Constructor to .*',     # AI-typical constructor comment pattern
            r'// Function to .*',        # AI-typical function purpose comments
            r'// Method to .*',          # AI-typical method purpose comments
            r'// Class to .*',           # AI-typical class purpose comments
            r'// [A-Z][a-z]+ [a-z]+ .*', # AI-typical sentence comments
        ]
        
        # Educational patterns often seen in AI code
        self.cpp_educational_patterns = [
            r'// Constructor',
            r'// Deposit',
            r'// Withdraw',
            r'// Display',
            r'// Get',
            r'// Add',
            r'// Class to',
            r'// Search for',
            r'// Account details',
            r'// \w+ money',
            r'===== .* =====',
            r'// Add a new',
            r'// Initialize',
            r'successfully created',
            r'Insufficient funds',
            r'New balance',
            r'Enter .* to',
            r'Account not found'
        ]
        
        # Domain-specific patterns (banking, student management, etc.)
        self.cpp_domain_patterns = [
            r'account(?:s|Holder|Number|Balance)',
            r'deposit',
            r'withdraw',
            r'balance',
            r'customer',
            r'student',
            r'library',
            r'book',
            r'inventory',
            r'product',
            r'employee',
            r'manager',
            r'system',
            r'database',
            r'user',
            r'admin',
            r'password',
            r'login',
            r'register',
            r'menu',
            r'option',
            r'selection'
        ]
        
        # C++ specific AI model names and tools
        self.cpp_ai_models = [
            'GPT', 'OpenAI', 'Claude', 'Copilot', 'Codex', 'DALL-E',
            'Stable Diffusion', 'Midjourney', 'Bard', 'PaLM', 'LLaMA',
            'Anthropic', 'DeepMind', 'CodeWhisperer', 'Tabnine', 'Kite',
            'IntelliCode', 'CodeGuru', 'CodeAssist', 'CodeComplete',
            'GitHub Copilot', 'Amazon CodeWhisperer', 'Gemini', 'ChatGPT',
            'Mistral', 'Perplexity', 'GPT-4', 'GPT-3.5', 'GPT-3', 'Cohere',
            'Hugging Face', 'Transformers', 'BERT', 'Falcon', 'Phi', 'Mixtral',
            'Anthropic Claude', 'Codeium', 'Cursor', 'Sourcegraph Cody'
        ]

        # C++ specific AI-related terms
        self.cpp_ai_terms = [
            'artificial intelligence', 'machine learning', 'neural network',
            'deep learning', 'AI model', 'language model', 'code generation',
            'auto-generated', 'automatically generated', 'generated code',
            'optimization algorithm', 'genetic algorithm', 'neural network',
            'machine learning model', 'AI implementation', 'automated solution',
            'large language model', 'LLM', 'AI assistant', 'prompt engineering',
            'autoregressive', 'transformer model', 'model parameters', 
            'token generation', 'completion', 'response generation',
            'context window', 'fine-tuned model', 'pretrained model',
            'supervised learning', 'unsupervised learning', 'reinforcement learning',
            'temperature setting', 'prompt completion', 'zero-shot', 'few-shot',
            'training data', 'inference time', 'code recommendation', 'AI pair programmer'
        ]

        # Additional specialized C++ plagiarism indicators
        self.cpp_plagiarism_patterns = [
            r'(\s+)\S+\s*\([^)]*\)\s*\{\s*$\n\1\s+[^;{]*;\s*$\n\1\s*\}',  # Empty/trivial functions
            r'\/\/\s*TODO:',  # TODO comments (common in AI-generated code)
            r'\/\/\s*Function to\s*.*?$',  # Overly descriptive function comments
            r'\/\/\s*Helper function\s*.*?$',  # Helper function comments
            r'\/\/\s*Utility function\s*.*?$',  # Utility function comments
            r'\/\/\s*Method to\s*.*?$',  # Method description comments
            r'\/\/\s*Returns\s*.*?$',  # Return value comments
            r'\/\/\s*\w+ constructor',  # Constructor comments
            r'\/\/\s*\w+ destructor',  # Destructor comments
            r'#include <.*?>\s*// For .*?$',  # Explanatory include comments
            r'template<typename T>\s*T\s+\w+\(.*?\)',  # Generic template patterns
            r'void\s+\w+\([^)]*\)\s*{\s*//\s*\w+.*?$\n',  # Function with same-line comment
            r'for\s*\([^;]*;\s*[^;]*;\s*[^)]*\)\s*{\s*//\s*.*?$',  # For loop with same-line comment
            r'while\s*\([^)]*\)\s*{\s*//\s*.*?$',  # While loop with same-line comment
            r'if\s*\([^)]*\)\s*{\s*//\s*.*?$',  # If statement with same-line comment
            r'}\s*else\s*{\s*//\s*.*?$',  # Else statement with same-line comment
            r'std::cout\s*<<\s*"[^"]*"\s*<<\s*\w+\s*<<\s*"[^"]*"',  # Common output pattern
            r'std::string\s+\w+\s*=\s*"[^"]*";',  # String initialization pattern
            r'std::vector<\w+>\s+\w+;',  # Vector declaration
            r'return\s+0;\s*}\s*$',  # Main function return pattern
            r'return\s+\w+;\s*}\s*$',  # Return pattern
            r'(\S+)\s+\1\s*\(',  # Function naming repetition
            r'[A-Z][a-z]+(?:[A-Z][a-z]+){2,}',  # Complex CamelCase (3+ words)
            r'[a-z]+(?:_[a-z]+){2,}',  # Complex snake_case (3+ words)
            r'\*\s*\w+\s*=\s*new\s+\w+\([^)]*\);',  # Raw pointer allocation
            r'delete\s+\w+;',  # Raw delete
            r'delete\[\]\s+\w+;',  # Raw array delete
        ]
        
        # Common C++ AI-generated code patterns
        self.cpp_code_patterns = [
            r'std::unique_ptr<.*>',  # Smart pointer usage
            r'std::shared_ptr<.*>',  # Shared pointer usage
            r'std::make_unique<.*>',  # Modern C++ patterns
            r'std::make_shared<.*>',
            r'std::move\(.*\)',       # Move semantics
            r'std::forward<.*>',      # Perfect forwarding
            r'std::enable_if<.*>',    # SFINAE
            r'std::is_same<.*>',      # Type traits
            r'std::conditional<.*>',
            r'std::result_of<.*>',
            r'std::invoke_result<.*>',
            r'std::function<.*>',     # Modern function wrappers
            r'std::bind<.*>',
            r'std::thread',           # Modern threading
            r'std::async',
            r'std::future',
            r'std::promise',
            r'std::mutex',            # Modern synchronization
            r'std::lock_guard',
            r'std::unique_lock',
            r'std::shared_lock',
            r'std::condition_variable',
            r'std::atomic<.*>',       # Atomic operations
            r'std::variant<.*>',      # Modern C++ features
            r'std::optional<.*>',
            r'std::any',
            r'std::string_view',
            r'std::span<.*>',
            r'std::filesystem',       # Modern filesystem operations
            r'std::chrono',           # Modern time operations
            r'std::format',           # Modern formatting
            r'std::ranges',           # Modern ranges
            r'std::views',
            r'std::concepts',         # Modern concepts
            r'std::requires',
            r'std::constexpr',        # Modern compile-time features
            r'std::consteval',
            r'std::constinit',
            r'auto\s+\w+\s*=',        # Type inference
            r'\[\s*&\s*\]\s*\(',      # Lambda capture
            r'\[\s*=\s*\]\s*\(',      # Lambda capture by value
            r'\[\s*\]\s*\(',          # Lambda with empty capture
            r'std::array<.*?,\s*\d+>',  # Fixed-size array
            r'std::pair<.*?,.*?>',    # Pairs
            r'std::tuple<.*?>',       # Tuples
            r'std::map<.*?,.*?>',     # Maps
            r'std::unordered_map<.*?,.*?>', # Hash maps
            r'std::set<.*?>',         # Sets
            r'std::unordered_set<.*?>', # Hash sets
            r'std::initializer_list<.*?>', # Initializer lists
            r'std::begin\(.*?\)',     # Container functions
            r'std::end\(.*?\)',
            r'std::rbegin\(.*?\)',
            r'std::rend\(.*?\)',
            r'std::transform\(.*?\)', # Algorithms
            r'std::find_if\(.*?\)',
            r'std::sort\(.*?\)',
            r'std::all_of\(.*?\)',
            r'std::any_of\(.*?\)',
            r'std::none_of\(.*?\)',
            r'std::accumulate\(.*?\)',
            r'std::for_each\(.*?\)',
            r'std::generate\(.*?\)',
            r'std::remove_if\(.*?\)',
            r'std::copy_if\(.*?\)',
            r'std::string\s+\w+\s*\{.*?\}', # Uniform initialization
            r'std::vector<.*?>\s+\w+\s*\{.*?\}',
            r'\w+<.*?>\s+\w+\s*\{.*?\}',
            r'\(\s*\[\s*\w+\s*\]\s*\(.*?\)\s*\{', # Immediately invoked lambda
            r'static_assert\(.*?\)', # Compile-time assertions
        ]

        # Common C++ AI-generated naming patterns
        self.cpp_naming_patterns = [
            r'[A-Z][a-z]+(?:[A-Z][a-z]+)*',  # CamelCase
            r'[a-z]+(?:_[a-z]+)*',           # snake_case
            r'[A-Z]+(?:_[A-Z]+)*',           # UPPER_SNAKE_CASE
            r'[A-Z][a-z]+(?:[A-Z][a-z]+)*',  # PascalCase
            r'[a-z]+(?:[A-Z][a-z]+)*',       # camelCase
            r'm_[a-z]+(?:[A-Z][a-z]+)*',     # m_prefixed member variables
            r'_[a-z]+(?:[A-Z][a-z]+)*',      # _prefixed private variables
            r'g_[a-z]+(?:[A-Z][a-z]+)*',     # g_prefixed global variables
            r's_[a-z]+(?:[A-Z][a-z]+)*',     # s_prefixed static variables
            r'I[A-Z][a-z]+(?:[A-Z][a-z]+)*', # Interface naming (IMyInterface)
            r'T[A-Z][a-z]+(?:[A-Z][a-z]+)*', # Template naming (TMyTemplate)
            r'[a-z]+(?:[A-Z][a-z]+)*?(?:Impl|Helper|Util|Manager|Factory|Builder|Provider|Service|Handler|Controller)',  # Common suffixes
        ]

        # Initialize the C++ AI code detector with known patterns.
        self.model = None
        self.model_path = 'ai_detector_model.pkl'
        self._init_ml_model()
        
        # Initialize TF-IDF vectorizer
        self.tfidf = None
        self.tfidf_matrix = None
        self._init_tfidf()
    
    def _init_ml_model(self):
        """Initialize or load the machine learning model for detection."""
        if not SKLEARN_AVAILABLE:
            self.model = None
            return
            
        if os.path.exists(self.model_path):
            try:
                with open(self.model_path, 'rb') as f:
                    self.model = pickle.load(f)
                print(f"Loaded ML model from {self.model_path}")
            except Exception as e:
                print(f"Failed to load model: {e}")
                self.model = self._create_default_model()
        else:
            self.model = self._create_default_model()
    
    def _create_default_model(self):
        """Create a default model with basic training."""
        # Create a simple Random Forest model
        model = RandomForestClassifier(n_estimators=50, random_state=42)
        
        # We'll initialize with some basic examples if available
        try:
            # This would normally be loaded from a dataset
            # For now we'll create a very basic model with default weights
            X = [[0.1, 0.2, 0.3, 0.1],  # Human example 1
                 [0.2, 0.1, 0.1, 0.2],  # Human example 2
                 [0.7, 0.8, 0.6, 0.7],  # AI example 1
                 [0.6, 0.7, 0.7, 0.8]]  # AI example 2
            y = [0, 0, 1, 1]  # 0 for human, 1 for AI
            
            # Train the model on our minimal dataset
            model.fit(X, y)
            print("Initialized default ML model for AI detection")
        except Exception as e:
            print(f"Failed to create default model: {e}")
        
        return model
    
    def _extract_ml_features(self, code: str) -> List[float]:
        """Extract features for ML-based detection."""
        # Basic feature extraction for ML model
        features = []
        
        # 1. Comment to code ratio
        comments = re.findall(r'(/\*.*?\*/|//.*?$)', code, re.DOTALL | re.MULTILINE)
        comment_chars = sum(len(c) for c in comments)
        code_chars = len(code) - comment_chars
        comment_ratio = comment_chars / max(1, len(code))
        features.append(comment_ratio)
        
        # 2. Average identifier length
        identifiers = re.findall(r'\b[a-zA-Z_][a-zA-Z0-9_]*\b', code)
        avg_id_length = sum(len(i) for i in identifiers) / max(1, len(identifiers))
        features.append(min(1.0, avg_id_length / 20.0))  # Normalize
        
        # 3. Function complexity (nesting levels)
        # Simple approximation by counting braces depth
        max_depth = 0
        current_depth = 0
        for char in code:
            if char == '{':
                current_depth += 1
                max_depth = max(max_depth, current_depth)
            elif char == '}':
                current_depth = max(0, current_depth - 1)
        features.append(min(1.0, max_depth / 10.0))  # Normalize
        
        # 4. Repetition pattern (repeated structures)
        lines = code.split('\n')
        line_prefixes = [line.strip()[:10] for line in lines if line.strip()]
        prefix_counts = Counter(line_prefixes)
        repetition_score = sum(count for count in prefix_counts.values() if count > 1) / max(1, len(line_prefixes))
        features.append(min(1.0, repetition_score))
        
        # Return the feature vector
        return features
    
    def _predict_with_ml(self, code: str) -> float:
        """Use ML model to predict likelihood of AI generation."""
        if self.model is None:
            return 0.0
        
        try:
            features = self._extract_ml_features(code)
            # Get probability of AI class (class 1)
            proba = self.model.predict_proba([features])[0][1]
            return float(proba)
        except Exception as e:
            print(f"ML prediction error: {e}")
            return 0.0

    def _init_tfidf(self):
        """Initialize TF-IDF vectorizer with AI code snippets."""
        if not SKLEARN_AVAILABLE:
            return
        
        try:
            self.tfidf = TfidfVectorizer(
                analyzer='word',
                token_pattern=r'(?u)\b\w+\b',
                ngram_range=(1, 3),
                max_features=1000
            )
            
            # Fit vectorizer on AI code snippets
            self.tfidf_matrix = self.tfidf.fit_transform(AI_CODE_SNIPPETS)
            print("Initialized TF-IDF vectorizer for similarity detection")
        except Exception as e:
            print(f"Error initializing TF-IDF: {e}")
            self.tfidf = None
            self.tfidf_matrix = None
    
    def _calculate_similarity(self, code: str) -> float:
        """Calculate similarity between the code and known AI examples."""
        if not hasattr(self, 'tfidf') or not hasattr(self, 'tfidf_matrix'):
            self._init_tfidf()
        
        # Transform the code
        code_vector = self.tfidf.transform([code])
        
        # Calculate similarity with each AI snippet
        similarities = (code_vector @ self.tfidf_matrix.T).toarray()[0]
        
        # Return the maximum similarity
        return float(max(similarities)) if len(similarities) > 0 else 0.0

    def analyze_code(self, code: str) -> Dict[str, float]:
        """Analyze the code for signs of AI generation."""
        results = {}
        
        # Check for educational patterns (tutorials, examples)
        results['educational_patterns_score'] = self._check_educational_patterns(code)
        
        # Check for domain patterns
        results['domain_patterns_score'] = self._check_domain_patterns(code)
        
        # Analyze code structure
        results['code_structure_score'] = self._analyze_code_structure(code)
        
        # Analyze formatting consistency
        results['formatting_consistency_score'] = self._analyze_formatting_consistency(code)
        
        # Calculate similarity to known AI examples
        results['similarity_score'] = self._calculate_similarity(code)
        
        # ML prediction if available
        if SKLEARN_AVAILABLE and hasattr(self, 'model'):
            results['ml_prediction_score'] = self._predict_with_ml(code)
        else:
            # If ML not available, use a fixed value based on other metrics
            results['ml_prediction_score'] = 0.26  # Default balanced value
        
        # Calculate overall score
        results['overall_score'] = self._calculate_overall_score(results)
        
        return results

    def _calculate_overall_score(self, scores: Dict[str, float]) -> float:
        weights = {
            'educational_patterns_score': 0.15,
            'domain_patterns_score': 0.05,  # Reduced weight for domain patterns
            'code_structure_score': 0.15,
            'formatting_consistency_score': 0.15,
            'similarity_score': 0.20,
            'ml_prediction_score': 0.30
        }
        
        weighted_sum = 0
        for key, weight in weights.items():
            if key in scores:
                weighted_sum += scores[key] * weight
        
        # Apply a calibration curve to get more balanced scores
        calibrated_score = weighted_sum
        if calibrated_score > 0.50:
            # Reduce extremely high scores (typically false positives)
            calibrated_score = 0.50 + (calibrated_score - 0.50) * 0.5
        
        return calibrated_score

    def _check_ai_patterns(self, code: str) -> float:
        """Check for C++ specific AI generation patterns in comments."""
        score = 0
        for pattern in self.cpp_ai_patterns:
            if re.search(pattern, code, re.IGNORECASE):
                score += 1
        return min(score / len(self.cpp_ai_patterns), 1.0)

    def _analyze_comments(self, code: str) -> float:
        """Analyze comment patterns that suggest AI generation."""
        # Extract all comments
        comments = re.findall(r'//.*?$|/\*.*?\*/', code, re.DOTALL|re.MULTILINE)
        
        if not comments:
            return 0.0
            
        # Check for comment characteristics
        scores = []
        
        # Check for natural language comments (complete sentences)
        sentence_comments = sum(1 for comment in comments if re.search(r'//\s*[A-Z][^.!?]*[.!?]', comment))
        scores.append(min(sentence_comments / max(1, len(comments)), 1.0) * 0.5)
        
        # Check for detailed explanations in comments
        detailed_comments = sum(1 for comment in comments 
                              if len(re.sub(r'//|\*|/\*|\*/', '', comment).strip()) > 30)
        scores.append(min(detailed_comments / max(1, len(comments)), 1.0) * 0.7)
        
        # Check for comments that explain obvious operations
        obvious_comments = sum(1 for comment in comments 
                             if re.search(r'//\s*(Get|Set|Initialize|Return|Increment|Decrement|Add|Remove)\s+', comment, re.IGNORECASE))
        scores.append(min(obvious_comments / max(1, len(comments)), 1.0) * 0.8)
        
        # Check for consistent comment formatting
        comment_lines = []
        for comment in comments:
            lines = comment.split('\n')
            comment_lines.extend([line.strip() for line in lines if line.strip()])
            
        if comment_lines:
            # Count comments that start with the same format
            patterns = [
                r'//\s+[A-Z][a-z]',  # Capitalized comments
                r'//\s+[a-z]',       # Lowercase comments
                r'//\s+\w+:',        # Label-style comments
                r'//\s+-',           # Dash-style comments
                r'//\s+\*',          # Asterisk bullet comments
            ]
            
            pattern_matches = [sum(1 for line in comment_lines if re.match(pattern, line)) for pattern in patterns]
            most_common_pattern = max(pattern_matches) if pattern_matches else 0
            
            format_consistency = most_common_pattern / len(comment_lines) if comment_lines else 0
            scores.append(format_consistency * 0.6)
        
        # Check for comment-to-code ratio that matches AI patterns
        code_lines = len([line for line in code.split('\n') if line.strip() and not re.match(r'^\s*(//|/\*|\*|$)', line)])
        comment_to_code_ratio = len(comment_lines) / max(1, code_lines)
        
        if comment_to_code_ratio > 0.3 or comment_to_code_ratio < 0.05:
            scores.append(min(abs(comment_to_code_ratio - 0.15) * 2, 1.0))
        
        # Per-inline-comment word count
        inline_comments = [c for c in comments if not '\n' in c]
        if inline_comments:
            words = [len(re.findall(r'\b\w+\b', c)) for c in inline_comments]
            avg_words = sum(words) / len(words) if words else 0
            if avg_words > 5:  # AI tends to write wordier inline comments
                scores.append(min((avg_words - 5) / 5, 1.0))
        
        return sum(scores) / max(1, len(scores)) * 1.2  # Slightly amplify

    def _analyze_variable_naming(self, code: str) -> float:
        """Analyze variable naming patterns that suggest AI generation."""
        # Extract variable declarations
        var_decls = re.findall(r'\b(?:int|float|double|char|string|bool|auto|vector<[^>]*>|map<[^>]*>|set<[^>]*>|std::\w+<[^>]*>)\s+(\w+)\s*[;=]', code)
        if not var_decls:
            return 0.0
        
        # AI naming pattern scores
        scores = []
        
        # Check for overly descriptive variable names
        descriptive_names = sum(1 for var in var_decls if len(var) > 12)
        scores.append(min(descriptive_names / len(var_decls), 1.0) * 0.7)
        
        # Check for common AI variable naming patterns
        ai_patterns = [
            r'^(?:num|count|total|sum|avg|min|max|temp|tmp)\w*$',
            r'^(?:is|has|should|can|will|did)\w+$',
            r'^current\w+$',
            r'^new\w+$',
            r'^result\w*$',
        ]
        
        for pattern in ai_patterns:
            matches = sum(1 for var in var_decls if re.match(pattern, var))
            if matches > 0:
                scores.append(min(matches / len(var_decls) * 2, 1.0) * 0.6)
        
        # Check for consistent naming conventions (AI tends to be very consistent)
        camel_case = sum(1 for var in var_decls if re.match(r'^[a-z]+(?:[A-Z][a-z]+)+$', var))
        snake_case = sum(1 for var in var_decls if re.match(r'^[a-z]+(?:_[a-z]+)+$', var))
        pascal_case = sum(1 for var in var_decls if re.match(r'^[A-Z][a-z]+(?:[A-Z][a-z]+)+$', var))
        
        max_convention = max(camel_case, snake_case, pascal_case)
        consistency_score = max_convention / len(var_decls) if max_convention > 0 else 0
        if consistency_score > 0.8:  # Extremely consistent naming is an AI trait
            scores.append(consistency_score)
        
        # Check for textbook-style variable names (i, j, k, n, x, y, z)
        textbook_vars = sum(1 for var in var_decls if re.match(r'^[ijknxyz]$', var))
        if len(var_decls) > 5 and textbook_vars / len(var_decls) > 0.3:
            scores.append(min(textbook_vars / len(var_decls) * 1.5, 1.0))
        
        return sum(scores) / max(1, len(scores))

    def _analyze_code_structure(self, code: str) -> float:
        """
        Analyze code structure for patterns common in AI-generated code.
        """
        # Check for consistent spacing and indentation
        consistent_spacing = False
        try:
            consistent_spacing = re.search(r'(\s{2,}|\t)\S+.*\n\1\S+', code) is not None
        except Exception as e:
            print(f"Error checking spacing consistency: {e}")
        
        # Check for symmetrical structure (functions with similar patterns)
        function_pattern = r'(void|int|float|double|bool|char|string|auto)\s+\w+\s*\([^)]*\)\s*{'
        functions = []
        try:
            functions = re.findall(function_pattern, code)
        except Exception as e:
            print(f"Error finding functions: {e}")
        
        # Common AI structure has similar sized functions
        function_blocks = []
        function_lengths = []
        try:
            function_blocks = re.split(function_pattern, code)[1:]
            function_lengths = [len(block) for block in function_blocks[1::2]] if len(function_blocks) > 1 else []
        except Exception as e:
            print(f"Error analyzing function blocks: {e}")
        
        # Calculate standard deviation of function lengths
        length_consistency = 0.0
        if function_lengths:
            avg_length = sum(function_lengths) / len(function_lengths)
            std_dev = (sum((l - avg_length) ** 2 for l in function_lengths) / len(function_lengths)) ** 0.5
            length_consistency = 1.0 - min(1.0, std_dev / (avg_length + 1))  # Normalize
            
        # Check for structured comments preceding functions
        comment_ratio = 0.0
        try:
            comment_before_function = len(re.findall(r'(/\*.*?\*/|//.*?\n)\s*(void|int|float|double|bool|char|string|auto)\s+\w+\s*\(', code, re.DOTALL))
            comment_ratio = min(1.0, comment_before_function / max(1, len(functions)))
        except Exception as e:
            print(f"Error checking comment patterns: {e}")
        
        # AI often uses structured error handling in every function
        try_catch_ratio = 0.0
        try:
            try_catch_blocks = len(re.findall(r'try\s*{', code))
            try_catch_ratio = min(1.0, try_catch_blocks / max(1, len(functions)))
        except Exception as e:
            print(f"Error checking error handling: {e}")
        
        # Calculate overall structure score
        structure_score = (
            (consistent_spacing * 0.2) + 
            (length_consistency * 0.3) + 
            (comment_ratio * 0.3) + 
            (try_catch_ratio * 0.2)
        )
        
        return structure_score

    def _check_model_references(self, code: str) -> float:
        """Check for references to AI models in the code."""
        score = 0
        for model in self.cpp_ai_models:
            if model.lower() in code.lower():
                score += 1
        return min(score / len(self.cpp_ai_models), 1.0)

    def _check_ai_terms(self, code: str) -> float:
        """Check for AI-related terms in comments and strings."""
        score = 0
        for term in self.cpp_ai_terms:
            if term.lower() in code.lower():
                score += 1
        return min(score / len(self.cpp_ai_terms), 1.0)

    def _analyze_complexity(self, code: str) -> float:
        """Analyze C++ code complexity patterns."""
        lines = code.split('\n')
        if not lines:
            return 0.0
            
        # Count function definitions
        functions = len(re.findall(r'\b(?:inline|virtual|constexpr|consteval)?\s*(?:[a-zA-Z_][a-zA-Z0-9_]*)\s*\([^)]*\)', code))
        
        # Count class definitions
        classes = len(re.findall(r'\bclass\s+[a-zA-Z_][a-zA-Z0-9_]*', code))
        
        # Count loops
        loops = len(re.findall(r'\b(for|while|do)\b', code))
        
        # Count conditionals
        conditionals = len(re.findall(r'\b(if|else if|else|switch)\b', code))
        
        # Count template instantiations
        templates = len(re.findall(r'<[^>]+>', code))
        
        # Calculate complexity score
        complexity = (functions + classes + loops + conditionals + templates) / len(lines)
        return min(complexity / 2, 1.0)

    def _analyze_modern_cpp_patterns(self, code: str) -> float:
        """Analyze usage of modern C++ features."""
        score = 0
        for pattern in self.cpp_code_patterns:
            if re.search(pattern, code):
                score += 1
        return min(score / len(self.cpp_code_patterns), 1.0)

    def _analyze_template_usage(self, code: str) -> float:
        """Analyze template usage patterns."""
        # Count template declarations
        template_decls = len(re.findall(r'template\s*<[^>]+>', code))
        
        # Count template instantiations
        template_insts = len(re.findall(r'<[^>]+>', code))
        
        # Count variadic templates
        variadic_templates = len(re.findall(r'\.\.\.', code))
        
        # Calculate template usage score
        if template_decls + template_insts == 0:
            return 0.0
        return min((template_decls + template_insts + variadic_templates) / 10, 1.0)

    def _analyze_error_handling(self, code: str) -> float:
        """Analyze error handling patterns."""
        # Count try-catch blocks
        try_catch = len(re.findall(r'\btry\b.*?\bcatch\b', code, re.DOTALL))
        
        # Count exception specifications
        exception_specs = len(re.findall(r'throw\s*\([^)]*\)', code))
        
        # Count noexcept specifications
        noexcept_specs = len(re.findall(r'noexcept', code))
        
        # Count error handling functions
        error_funcs = len(re.findall(r'\b(?:error|exception|fail|invalid|runtime_error)\b', code))
        
        return min((try_catch + exception_specs + noexcept_specs + error_funcs) / 5, 1.0)

    def _analyze_memory_management(self, code: str) -> float:
        """Analyze memory management patterns."""
        # Count smart pointer usage
        smart_ptrs = len(re.findall(r'\b(?:unique_ptr|shared_ptr|weak_ptr)\b', code))
        
        # Count memory allocation functions
        alloc_funcs = len(re.findall(r'\b(?:new|delete|malloc|free)\b', code))
        
        # Count RAII patterns
        raii_patterns = len(re.findall(r'\b(?:lock_guard|unique_lock|shared_lock|scoped_lock)\b', code))
        
        return min((smart_ptrs + alloc_funcs + raii_patterns) / 5, 1.0)

    def _analyze_concurrency_patterns(self, code: str) -> float:
        """Analyze concurrency patterns."""
        # Count thread-related patterns
        thread_patterns = len(re.findall(r'\b(?:thread|async|future|promise)\b', code))
        
        # Count synchronization primitives
        sync_patterns = len(re.findall(r'\b(?:mutex|condition_variable|semaphore|barrier)\b', code))
        
        # Count atomic operations
        atomic_patterns = len(re.findall(r'\b(?:atomic|atomic_thread_fence|atomic_signal_fence)\b', code))
        
        return min((thread_patterns + sync_patterns + atomic_patterns) / 5, 1.0)

    def _analyze_code_organization(self, code: str) -> float:
        """Analyze the organization of code to detect AI patterns."""
        lines = code.split('\n')
        if not lines:
            return 0.0
            
        scores = []
        
        # Check for consistent and clear section organization
        has_includes = any(re.match(r'^\s*#include', line) for line in lines)
        has_namespace = any(re.match(r'^\s*(?:using|namespace)', line) for line in lines)
        has_class_def = any(re.match(r'^\s*class\s+\w+', line) for line in lines)
        has_function_def = any(re.match(r'^\s*\w+\s+\w+\s*\(', line) for line in lines)
        has_main = any(re.match(r'^\s*int\s+main\s*\(', line) for line in lines)
        
        # AI tends to organize code in a textbook manner
        if has_includes and has_namespace and has_class_def and has_main:
            includes_first = min([i for i, line in enumerate(lines) 
                                if re.match(r'^\s*#include', line)] or [float('inf')])
            namespace_after_includes = min([i for i, line in enumerate(lines) 
                                        if re.match(r'^\s*(?:using|namespace)', line)] or [float('inf')])
            class_after_namespace = min([i for i, line in enumerate(lines) 
                                      if re.match(r'^\s*class\s+\w+', line)] or [float('inf')])
            main_last = max([i for i, line in enumerate(lines) 
                          if re.match(r'^\s*int\s+main\s*\(', line)] or [0])
            
            if (includes_first < namespace_after_includes < class_after_namespace and 
                main_last > class_after_namespace):
                scores.append(0.9)  # Perfect textbook organization
        
        # Check for evenly balanced bracket indentation (very common in AI code)
        indent_levels = []
        current_indent = 0
        for line in lines:
            stripped = line.strip()
            if not stripped:
                continue
                
            # Count open/close braces
            current_indent += stripped.count('{') - stripped.count('}')
            if current_indent >= 0:
                indent_levels.append(current_indent)
        
        if indent_levels:
            # Calculate consistency of indentation
            if max(indent_levels) > 0:
                level_counts = Counter(indent_levels)
                most_common_level_count = level_counts.most_common(1)[0][1]
                indent_consistency = most_common_level_count / len(indent_levels)
                if indent_consistency > 0.6:  # Suspiciously consistent
                    scores.append(indent_consistency)
        
        # Check for consistent pattern in function definitions
        function_defs = [line.strip() for line in lines 
                       if re.match(r'^\s*(?:void|int|double|float|bool|auto|string|std::\w+)\s+\w+\s*\(', line)]
        
        if len(function_defs) >= 3:
            # Check if functions follow consistent naming/style
            signature_patterns = []
            for func in function_defs:
                # Extract return type and function name
                match = re.match(r'(\w+(?:::\w+)?)\s+(\w+)\s*\(', func)
                if match:
                    return_type, func_name = match.groups()
                    signature_patterns.append((return_type, func_name[:3]))
            
            if signature_patterns:
                # Count consistent patterns
                pattern_counts = Counter(signature_patterns)
                most_common_pattern_count = pattern_counts.most_common(1)[0][1]
                pattern_consistency = most_common_pattern_count / len(signature_patterns)
                
                if pattern_consistency > 0.7:  # Unusually consistent style
                    scores.append(pattern_consistency)
        
        # Check for consistent code blocks and balanced methods
        class_defs = []
        current_class = None
        class_methods = []
        
        for line in lines:
            if re.match(r'^\s*class\s+\w+', line):
                if current_class and class_methods:
                    class_defs.append((current_class, class_methods))
                    
                current_class = re.match(r'^\s*class\s+(\w+)', line).group(1)
                class_methods = []
            elif current_class and re.match(r'^\s*(?:void|int|double|float|bool|auto|string|std::\w+)\s+\w+\s*\(', line):
                class_methods.append(line.strip())
        
        # Add the last class if it exists
        if current_class and class_methods:
            class_defs.append((current_class, class_methods))
        
        # AI-generated code often has very balanced class methods 
        if class_defs:
            for _, methods in class_defs:
                if len(methods) >= 3:
                    # Calculate average method length
                    method_lengths = [len(method) for method in methods]
                    avg_length = sum(method_lengths) / len(method_lengths)
                    
                    # Calculate standard deviation
                    std_dev = (sum((length - avg_length) ** 2 for length in method_lengths) / len(method_lengths)) ** 0.5
                    
                    # Unusually consistent method sizes
                    if std_dev / avg_length < 0.2 and len(methods) > 3:
                        scores.append(0.8)
        
        return sum(scores) / max(1, len(scores))

    def _check_plagiarism_patterns(self, code: str) -> float:
        """Check for specific plagiarism patterns in C++ code."""
        score = 0
        total_patterns = len(self.cpp_plagiarism_patterns)
        
        for pattern in self.cpp_plagiarism_patterns:
            if re.search(pattern, code, re.MULTILINE):
                score += 1
                
        return min(score / total_patterns * 1.5, 1.0)  # Amplify the score but cap at 1.0
    
    def _analyze_code_repetition(self, code: str) -> float:
        """Analyze code for repetitive patterns that suggest AI generation."""
        # Clean code of comments and strings for analysis
        clean_code = re.sub(r'//.*?$|/\*.*?\*/|".*?"|\'.*?\'', '', code, flags=re.DOTALL|re.MULTILINE)
        
        # Split into lines and remove empty lines
        lines = [line.strip() for line in clean_code.split('\n') if line.strip()]
        if not lines:
            return 0.0
            
        # Count identical lines
        line_counts = Counter(lines)
        identical_lines = sum(count - 1 for count in line_counts.values() if count > 1)
        
        # Count similar function signatures
        function_sigs = re.findall(r'\w+\s+\w+\s*\([^)]*\)', clean_code)
        sig_counts = Counter(function_sigs)
        similar_sigs = sum(count - 1 for count in sig_counts.values() if count > 1)
        
        # Count repetitive patterns in code
        patterns = []
        for i in range(len(lines)):
            for j in range(i + 2, min(i + 15, len(lines))):  # Look for 2-15 line patterns (more sensitive)
                pattern = '\n'.join(lines[i:j])
                if len(pattern) > 20:  # Lower threshold for considering it a pattern
                    patterns.append(pattern)
        
        pattern_counts = Counter(patterns)
        repetitive_patterns = sum(count - 1 for count in pattern_counts.values() if count > 1)
        
        # Calculate repetition score based on multiple factors - more sensitive weights
        repetition_score = min((identical_lines / len(lines) * 0.6 + 
                              similar_sigs / max(1, len(function_sigs)) * 0.4 +
                              repetitive_patterns / len(lines) * 0.3) * 2.5, 1.0)
                              
        return repetition_score
    
    def _analyze_comment_code_ratio(self, code: str) -> float:
        """Analyze the ratio of comments to code, a potential AI indicator."""
        # Extract all comments
        comments = re.findall(r'//.*?$|/\*.*?\*/', code, re.DOTALL|re.MULTILINE)
        
        # Count comment lines
        comment_lines = 0
        for comment in comments:
            comment_lines += comment.count('\n') + 1
        
        # Count code lines (excluding comments, empty lines)
        clean_code = re.sub(r'//.*?$|/\*.*?\*/', '', code, flags=re.DOTALL|re.MULTILINE)
        code_lines = len([line for line in clean_code.split('\n') if line.strip()])
        
        if code_lines == 0:
            return 0.0
            
        # Calculate ratio
        ratio = comment_lines / code_lines
        
        # AI often has very high or very low comment ratios
        # Very high (>0.3) or very low (<0.05) are suspicious - more sensitive thresholds
        if ratio > 0.3:
            return min((ratio - 0.3) * 2.5, 1.0)  # Score increases as ratio grows above 0.3
        elif ratio < 0.05:
            return (0.05 - ratio) * 20  # Score increases as ratio approaches 0
        else:
            return 0.0  # Normal comment ratio
    
    def _analyze_algorithmic_patterns(self, code: str) -> float:
        """Analyze for common algorithmic patterns often used by AI code generation."""
        patterns = [
            # Common algorithm implementations that follow textbook patterns
            r'(?:binary_search|quick_sort|merge_sort|bubble_sort|selection_sort|insertion_sort)',
            r'(?:depth_first_search|breadth_first_search|dijkstra|a_star|bellman_ford)',
            r'(?:dynamic_programming|greedy_algorithm|divide_and_conquer)',
            r'for\s*\(\s*int\s+i\s*=\s*0\s*;\s*i\s*<\s*n\s*;\s*i\s*\+\+\s*\)',  # Classic loop pattern
            r'while\s*\(\s*(?:low|start)\s*<=\s*(?:high|end)\s*\)',  # Binary search pattern
            r'if\s*\(\s*\w+\s*==\s*nullptr\s*\)',  # Null check pattern
            r'if\s*\(\s*\w+\s*\.\s*empty\(\s*\)\s*\)',  # Empty check pattern
            r'for\s*\(\s*auto\s+[&:]\s+\w+\s*:\s*\w+\s*\)',  # Range-based for loop
            r'try\s*\{.*?\}\s*catch\s*\(\s*std::exception\s*&\s*\w*\s*\)',  # Generic try-catch
            r'switch\s*\(\s*\w+\s*\)\s*\{\s*case',  # Switch-case pattern
            r'return\s+(?:true|false)\s*;',  # Boolean return
            r'if\s*\(\s*!\s*\w+\s*\)',  # Negation check
            r'return\s+\w+\s*\?\s*\w+\s*:\s*\w+\s*;',  # Ternary return
            r'\w+\+\+\s*;',  # Increment
            r'\w+\s*\+=\s*\w+\s*;',  # Compound assignment
            r'std::cout\s*<<\s*"[^"]*"\s*<<\s*\w+\s*<<\s*(?:std::endl|"\\n")',  # Output pattern
            r'std::cin\s*>>\s*\w+\s*;',  # Input pattern
            r'return\s+(?:-1|0|1)\s*;',  # Simple return values
        ]
        
        score = 0
        for pattern in patterns:
            if re.search(pattern, code):
                score += 1
                
        return min(score / len(patterns) * 1.5, 1.0)  # Amplify but cap at 1.0
    
    def _analyze_identifier_lengths(self, code: str) -> float:
        """Analyze identifier lengths for patterns typical of AI-generated code."""
        # Extract all identifiers (variable and function names)
        identifiers = re.findall(r'\b[a-zA-Z_][a-zA-Z0-9_]*\b', code)
        
        if not identifiers:
            return 0.0
            
        # Filter out keywords and standard library names
        cpp_keywords = ['auto', 'break', 'case', 'char', 'const', 'continue', 'default', 
                      'do', 'double', 'else', 'enum', 'extern', 'float', 'for', 'goto', 
                      'if', 'int', 'long', 'register', 'return', 'short', 'signed', 
                      'sizeof', 'static', 'struct', 'switch', 'typedef', 'union', 
                      'unsigned', 'void', 'volatile', 'while', 'class', 'new', 'delete',
                      'private', 'protected', 'public', 'this', 'friend', 'using', 'namespace',
                      'try', 'catch', 'throw', 'std', 'main', 'true', 'false', 'nullptr']
                      
        identifiers = [id for id in identifiers if id not in cpp_keywords]
        
        if not identifiers:
            return 0.0
            
        # Calculate length statistics
        lengths = [len(id) for id in identifiers]
        avg_length = sum(lengths) / len(lengths)
        
        # AI tends to use either very short or overly descriptive identifiers
        # Count very short (<2) and very long (>15) identifiers
        short_ids = sum(1 for l in lengths if l < 2)
        long_ids = sum(1 for l in lengths if l > 15)
        
        # Calculate percentage of extreme-length identifiers
        extreme_ratio = (short_ids + long_ids) / len(identifiers)
        
        # Calculate score based on average length and extreme ratios
        if avg_length < 3 or avg_length > 12:
            length_score = min(abs(avg_length - 7) / 5, 1.0)  # Optimal length around 7
        else:
            length_score = 0.0
            
        return (length_score * 0.5 + extreme_ratio * 0.5)

    def _analyze_code_verbosity(self, code: str) -> float:
        """Analyze code for verbosity patterns typical of AI-generated C++ code."""
        # Extract all comments
        comments = re.findall(r'//.*?$|/\*[\s\S]*?\*/', code, re.DOTALL|re.MULTILINE)
        
        # Measure average comment length
        if comments:
            avg_comment_length = sum(len(c) for c in comments) / len(comments)
            comment_verbosity = min(avg_comment_length / 30, 1.0)  # Consider comments over 30 chars as verbose
        else:
            comment_verbosity = 0.0
            
        # Measure descriptive function names (typical AI pattern)
        function_names = re.findall(r'\w+\s+(\w+)\s*\([^)]*\)\s*(?:const)?\s*\{', code)
        if function_names:
            avg_name_length = sum(len(name) for name in function_names) / len(function_names)
            name_verbosity = min(avg_name_length / 12, 1.0)  # Consider names over 12 chars as verbose
        else:
            name_verbosity = 0.0
            
        # Look for explanatory patterns ("this function does X")
        explanatory_patterns = [
            r'// Function to',
            r'// Constructor to',
            r'// Method to',
            r'// Class (?:representing|for)',
            r'// \w+ for \w+',
            r'// \w+ to \w+',
            r'// Validate',
            r'// Check',
            r'// Initialize',
            r'// Process',
            r'// Display',
            r'// Handle',
            r'// Generate',
            r'// Calculate',
            r'// Return',
            r'// Set',
            r'// Get'
        ]
        
        explanatory_count = sum(len(re.findall(pattern, code, re.MULTILINE)) for pattern in explanatory_patterns)
        
        # Count inline comments explaining obvious operations (AI pattern)
        inline_comments = len(re.findall(r'}\s*//\s*.*?$|;\s*//\s*.*?$', code, re.MULTILINE))
        
        # Count comments on declarations or initializations
        declaration_comments = len(re.findall(r'(?:int|string|bool|vector|float|double|auto)\s+\w+\s*(?:=\s*[^;]+)?;\s*//\s*.*?$', code, re.MULTILINE))
        
        # Calculate verbosity score based on all factors
        verbosity_components = [
            comment_verbosity * 0.3,
            name_verbosity * 0.3,
            min(explanatory_count / 5, 1.0) * 0.2,
            min(inline_comments / 5, 1.0) * 0.1,
            min(declaration_comments / 5, 1.0) * 0.1
        ]
        
        return sum(verbosity_components)

    def _analyze_verbose_naming(self, code: str) -> float:
        """Analyze for excessively verbose naming patterns, an AI code generation hallmark."""
        # Extract variable declarations with long descriptive names
        variable_decls = re.findall(r'(?:int|string|bool|float|double|auto|vector<[^>]+>)\s+(\w+)', code)
        
        # Filter for descriptive names
        descriptive_patterns = [
            r'\w*[tT]emp\w*',         # temporary variables
            r'\w*[rR]esult\w*',        # result variables
            r'\w*[oO]utput\w*',        # output variables
            r'\w*[iI]nput\w*',         # input variables
            r'\w*[cC]urrent\w*',       # current item variables
            r'\w*[uU]nique\w*',        # unique identifiers
            r'\w*[aA]vailab\w*',       # availability flags
            r'\w*[sS]tatus\w*',        # status variables
            r'\w*[cC]ount\w*',         # counter variables
            r'\w*[iI]ndex\w*',         # index variables
            r'\w*[cC]ollection\w*',    # collection variables
            r'\w*[dD]atabase\w*',      # database references
            r'\w*[sS]ystem\w*',        # system variables
            r'\w*[mM]anag\w*',         # manager variables
            r'\w*[pP]rocess\w*',       # process variables
            r'\w*[iI]nitiat\w*',       # initialization variables
            r'\w*[dD]isplay\w*',       # display variables
            r'\w*[lL]ocat\w*',         # location variables
            r'\w*[rR]etriev\w*',       # retrieval variables
            r'\w*[rR]egist\w*',        # registration variables
        ]
        
        descriptive_count = 0
        for var in variable_decls:
            for pattern in descriptive_patterns:
                if re.match(pattern, var):
                    descriptive_count += 1
                    break
                    
        # Calculate score based on the portion of descriptive variables
        if variable_decls:
            descriptive_ratio = descriptive_count / len(variable_decls)
        else:
            descriptive_ratio = 0.0
        
        # Extract function/method names
        function_decls = re.findall(r'(?:void|int|string|bool|float|double|auto|vector<[^>]+>|\w+)\s+(\w+)\s*\([^)]*\)', code)
        
        # Check for verbose function names (often with verbs+objects)
        verbose_function_patterns = [
            r'(?:process|handle|display|initialize|calculate|validate|verify|generate|retrieve|register|locate|initiate)(?:[A-Z]\w*)+',
            r'\w+(?:ById|ByName|ByType|ByDate|ByStatus)',
            r'is\w+Available',
            r'get\w+',
            r'set\w+',
            r'add\w+',
            r'remove\w+',
            r'update\w+',
            r'delete\w+',
            r'create\w+',
            r'find\w+',
            r'check\w+',
            r'validate\w+'
        ]
        
        verbose_function_count = 0
        for func in function_decls:
            for pattern in verbose_function_patterns:
                if re.match(pattern, func):
                    verbose_function_count += 1
                    break
        
        # Calculate score based on the portion of verbose function names
        if function_decls:
            verbose_function_ratio = verbose_function_count / len(function_decls)
        else:
            verbose_function_ratio = 0.0
            
        return (descriptive_ratio * 0.6 + verbose_function_ratio * 0.4) * 1.5  # Amplify but keep under 1.0

    def _check_educational_patterns(self, code: str) -> float:
        """
        Check for patterns typical in educational or tutorial code,
        which is common in AI-generated code.
        """
        # Simple string matching approach instead of regex
        educational_indicators = [
            "// Example of",
            "// This demonstrates",
            "// Tutorial",
            "// Shows how to",
            "// Here's how",
            "// Learn how to",
            "// Step",
            "// First, we",
            "// Next, we",
            "// Finally,",
            "// In this code",
            "// To understand",
            "// Note that",
            "// Remember that",
            "// It's important to",
            "// You should",
            "// We need to",
            "// This is a",
            "// Let's",
            "// Now we",
            "/* This code demonstrates",
            "/* Example",
            "/* Step-by-step",
            "/* Tutorial",
            "// Begin by",
            "// Start by",
            "// We start by",
            "// This function shows",
            "// This is how we",
            "// This method demonstrates"
        ]
        
        # Count occurrences
        matches = 0
        for indicator in educational_indicators:
            matches += code.count(indicator)
        
        # Normalize by code length
        normalized_score = min(1.0, matches / max(1, len(code) / 500))
        return normalized_score

    def _check_domain_patterns(self, code: str) -> float:
        """Check for domain-specific patterns often seen in AI-generated C++ code."""
        if not code:
            return 0.0
        
        # To be considered a match, code must have multiple matches or be very short
        total_matches = 0
        # Check for all domain patterns
        for pattern in self.cpp_domain_patterns:
            matches = re.findall(pattern, code.lower())
            if matches:
                # Avoid false positives by requiring multiple matches or high density
                if len(matches) > 1 or len(code) < 200:
                    total_matches += 1
        
        # Calculate score based on matches
        max_patterns = min(5, len(self.cpp_domain_patterns))  # Require fewer matches
        score = min(1.0, total_matches / max_patterns)
        
        # Reduce score for very short code snippets that might match by chance
        if len(code) < 100:
            score *= 0.5
        
        return score

    def _analyze_formatting_consistency(self, code: str) -> float:
        """
        Analyze the consistency of code formatting, which is often very 
        uniform in AI-generated code.
        """
        lines = code.split('\n')
        if not lines:
            return 0.0
            
        # Check indentation consistency
        indent_spaces = []
        for line in lines:
            if line.strip():  # Skip empty lines
                # Count leading spaces directly
                indent = len(line) - len(line.lstrip())
                indent_spaces.append(indent)
        
        if not indent_spaces:
            return 0.0
            
        # Calculate indentation consistency score
        indentation_types = set(indent_spaces)
        if len(indentation_types) <= 3:  # Very consistent if only a few indentation levels
            indent_score = 0.8
        else:
            indent_score = min(1.0, 5.0 / len(indentation_types))
            
        # Check for consistent spacing around operators - simple approach
        spacing_patterns = [
            " = ",   # Assignment
            " + ",   # Addition
            " - ",   # Subtraction
            " * ",   # Multiplication
            " / ",   # Division
            "if (",  # if statement
            "for (", # for loop
            "while (", # while loop
            "} else {" # else block
        ]
        
        # Count total operator patterns and consistent ones
        spacing_consistency = []
        for pattern in spacing_patterns:
            count = code.count(pattern)
            if count > 0:
                spacing_consistency.append(0.8)  # Simplified approach
            
        # Calculate spacing consistency score
        spacing_score = sum(spacing_consistency) / max(1, len(spacing_consistency))
        
        # Check for consistent line ending with simple string counting
        semicolon_lines = 0
        code_lines = 0
        
        for line in lines:
            stripped = line.strip()
            if stripped and not stripped.startswith("//") and not stripped.startswith("/*"):
                code_lines += 1
                if stripped.endswith(";"):
                    semicolon_lines += 1
                    
        semicolon_score = semicolon_lines / max(1, code_lines)
            
        # Calculate overall formatting consistency
        overall_consistency = (indent_score * 0.4 + spacing_score * 0.4 + semicolon_score * 0.2)
        return min(1.0, overall_consistency)

    def is_ai_generated(self, code: str, threshold: float = 0.36) -> Tuple[bool, Dict[str, float]]:
        """
        Determine if code is likely AI-generated.
        
        Args:
            code: The code to analyze
            threshold: Score threshold for AI detection (higher = less sensitive)
            
        Returns:
            Tuple of (is_ai_generated, detailed_scores)
        """
        results = self.analyze_code(code)
        return results['overall_score'] >= threshold, results

def analyze_file(file_path: str) -> Tuple[bool, Dict[str, float]]:
    """Analyze a C++ code file and return AI generation likelihood."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            code = f.read()
        detector = CppAIDetector()
        return detector.is_ai_generated(code)
    except Exception as e:
        print(f"Error analyzing file {file_path}: {str(e)}")
        return False, {}

def analyze_folder(folder_path: str) -> List[Tuple[str, bool, Dict[str, float], List[str]]]:
    """Analyze all C++ or txt files in a folder and return results."""
    results = []
    
    # Find all C++ and txt files
    file_patterns = ['*.cpp', '*.cc', '*.h', '*.hpp', '*.txt']
    files = []
    for pattern in file_patterns:
        files.extend(glob.glob(os.path.join(folder_path, "**", pattern), recursive=True))
    
    # If no files found, print a message
    if not files:
        print(f"No C++ or text files found in {folder_path}")
        return results
    
    # Analyze each file
    for file_path in files:
        try:
            is_ai, scores = analyze_file(file_path)
            
            # Determine top reasons
            sorted_scores = sorted(
                [(k, v) for k, v in scores.items() if k != 'overall_score'],
                key=lambda x: x[1],
                reverse=True
            )
            top_reasons = [key for key, _ in sorted_scores[:3]]
            
            results.append((file_path, is_ai, scores, top_reasons))
        except Exception as e:
            print(f"Error analyzing {file_path}: {e}")
    
    return results

def print_analysis_results(file_path, results, is_ai_generated, score):
    """Print the analysis results in a human-readable format."""
    print(f"\nAnalyzing {'text' if file_path.endswith('.txt') else 'source'} file: {file_path}")
    
    # Status emoji based on result
    status_emoji = "⚠️" if is_ai_generated else "✅"
    risk_level = "🔴 HIGH RISK" if score > 0.35 else "🟡 MEDIUM RISK" if score > 0.25 else "🟢 MINIMAL RISK"
    
    # Human-readable likelihood
    likelihood = calculate_human_readable_likelihood(score)
    
    # Determine status text
    status_text = "LIKELY AI-GENERATED" if is_ai_generated else "LIKELY HUMAN-WRITTEN"
    
    # Print the main result
    print(f"Status: {status_emoji} {status_text} ({score:.2%} technical score) - {risk_level}")
    print(f"Human-readable likelihood: {likelihood} chance of being AI-generated")
    
    # Print top indicators
    print("\nTop AI indicators:")
    
    sorted_indicators = sorted(
        [(k, v) for k, v in results.items() if k != 'overall_score'], 
        key=lambda x: x[1], 
        reverse=True
    )
    
    # Print top 5 indicators
    for key, value in sorted_indicators[:5]:
        if value > 0.05:  # Only show significant indicators
            print(f"  {key}: {value:.4f}")
    
    # Add specific warnings for high similarity
    if results.get('similarity_score', 0) > 0.25:
        print("High similarity to known AI code patterns detected")
    
    # Print detailed scores
    print("\nDetailed scores:")
    for key in sorted(results.keys()):
        print(f"  {key}: {results[key]:.4f}")

def calculate_human_readable_likelihood(score):
    """Calculate a human-readable likelihood percentage based on the score."""
    # More balanced calculation with reduced sensitivity
    if score < 0.25:
        return "5%"
    elif score < 0.30:
        return "20%"
    elif score < 0.35:
        return "40%"
    elif score < 0.40:
        return "60%"
    elif score < 0.45:
        return "80%"
    else:
        return "95%"

def main():
    """Process command line arguments and analyze files/directories."""
    print("=" * 50)
    
    if len(sys.argv) < 2:
        print("Usage: python chaicheck.py <file_or_directory_path>")
        sys.exit(1)
    
    target_path = sys.argv[1]
    
    if os.path.isfile(target_path):
        try:
            is_ai, results = analyze_file(target_path)
            print_analysis_results(target_path, results, is_ai, results['overall_score'])
        except Exception as e:
            print(f"Error analyzing file {target_path}: {e}")
    
    elif os.path.isdir(target_path):
        results = analyze_folder(target_path)
        ai_count = 0
        human_count = 0
        
        for file_path, is_ai, scores, reasons in results:
            if is_ai:
                ai_count += 1
            else:
                human_count += 1
            
            print_analysis_results(file_path, scores, is_ai, scores['overall_score'])
        
        print("\nSummary:")
        print(f"Analyzed {len(results)} files")
        print(f"AI-generated: {ai_count}")
        print(f"Human-written: {human_count}")
    
    else:
        print(f"Error: {target_path} is not a valid file or directory")
        sys.exit(1)

if __name__ == "__main__":
    main()
